Assignment 3.1: 1. Write a Map Reduce program to filter out the invalid records. Map only job will fit for this context. 
Commands:
	Step 1: create a directory inputFiles.
	--> hdfs dfs -mkdir -m 755 inputFiles
	Step 2: copy input file to hdfs folder inputFiles. 
	--> hdfs dfs -put /home/acadgild/inputFiles/television.txt inputFiles/
	Step 3: write mapreduce program in java. For code please look into below.
	Step 4: Create Jar File
	Step 5: Make sure there is no output folder. for that run delete command
	--> hdfs dfs -rm -r outputTVFilterInvalid/
	Step 6: run jar command
	--> hadoop jar /home/acadgild/workspace/TVSetsFilterInvalid.jar inputFiles/television.txt outputTVFilterInvalid/
	Step 7: If success open file
	--> hdfs dfs -ls outputTVFilterInvalid/
	Step 8: From 2 files data file format will be part-x-xxxxx. open that file.
	--> hdfs dfs -cat outputTVFilterInvalid/part-m-00000

JAVA Mapreduce program:
	This is map only program, so there is no reducer class.
	Mapper Class:

	package Sess3Assign2;

	import java.io.IOException;
	import org.apache.hadoop.io.LongWritable;
	import org.apache.hadoop.io.NullWritable;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapreduce.Mapper;

	public class TVSetsFilterInvalidMap extends Mapper<LongWritable, Text, Text, NullWritable>{	
	public void map(LongWritable key, Text value, Context context) 			
	throws IOException, InterruptedException 	
	{			
		String[] lineArray = value.toString().trim().split("\\|");		
		if(!"NA".equals(lineArray[0]) && !"NA".equals(lineArray[1]))//// this line is to filter invalid records contains 'NA'		
		{			
			context.write(new Text(value), NullWritable.get());		
		}	
	}

	}

	Driver Class:

	package Sess3Assign2;

	import java.util.Map;
	import org.apache.hadoop.conf.Configuration;
	import org.apache.hadoop.fs.Path;
	import org.apache.hadoop.io.Text;
	import org.apache.hadoop.mapreduce.Job;
	import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
	import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
	import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

	public class TVSetsFilterInvalidDriver  
	{
		@SuppressWarnings("deprecation")
		public static void main(String[] args) throws Exception {
			Configuration conf = new Configuration();
			Job job = new Job(conf, "TVSets Filter Invalid NA records");
			job.setJarByClass(TVSetsFilterInvalidDriver.class);

			job.setOutputKeyClass(Map.class);
			job.setOutputValueClass(Text.class);

			job.setMapperClass(TVSetsFilterInvalidMap.class);
			job.setNumReduceTasks(0);	//reducer doesnt required, so configuring as 0	

			job.setInputFormatClass(TextInputFormat.class);

			FileInputFormat.addInputPath(job, new Path(args[0])); 
			FileOutputFormat.setOutputPath(job,new Path(args[1]));

			job.waitForCompletion(true);
		}
	}


